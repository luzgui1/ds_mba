{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Exercício 2 - Aula 2**"
      ],
      "metadata": {
        "id": "b_lUc0-cLsVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dado o dataset de produtos [1], desenvolva os seguintes pipelines:\n",
        "\n",
        "[1] - https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\n",
        "\n",
        "Obs.: em todos os pipelines use:\n",
        "- normalização renovendo valores faltantes\n",
        "- criem uma nova coluna concatenando as colunas nome e descrição.\n",
        "- randon_state igual a 42 para permitir a comparação com seus colegas e separe uma amostra de 30% para teste.\n"
      ],
      "metadata": {
        "id": "i9jiVrQi-al0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pacotes utilizados\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# carregar dataframe\n",
        "df = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\", delimiter=\";\", encoding='utf-8')\n",
        "\n",
        "# limpeza inicial (normalização)\n",
        "df.dropna(inplace=True)\n",
        "df[\"texto\"] = df['nome'] + \" \" + df['descricao']\n",
        "\n",
        "# divisão da amostra entre treino e teste\n",
        "df_train, df_test = train_test_split(\n",
        "      df,\n",
        "      test_size = 0.3,\n",
        "      random_state = 42\n",
        "  )"
      ],
      "metadata": {
        "id": "XjpvUZj5IPpG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. ) Treine um modelo de classificação LogisticRegression do pacote scikit-learn para classificar os produtos em suas categorias, com as seguintes configurações:\n",
        "\n",
        "- 2.1 Contagem de termos simples com unigrama e com stop-words.\n",
        "- 2.2 Contagem de termos simples com unigrama + brigrama e sem stop-words.\n",
        "- 2.3 TF-IDF com unigrama e sem stop-words.\n",
        "- 2.4 TF-IDF com unigrama e sem stop-words em textos lematizados.\n",
        "\n",
        "Extra:\n",
        "- 2.5 Contagem de termos simples (BoW) com unigrama, sem stop-words (combinando Spacy e NLTK) em textos com apenas verbos lematizados.\n",
        "\n",
        "Dica: crie uma função para lematizar o texto usando o Spacy, não esqueça de usar o POS-Tag quando necessário."
      ],
      "metadata": {
        "id": "6I_PEQasSIUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 Contagem de termos simples com unigrama e com stop-words.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# vetorização\n",
        "vect = CountVectorizer(ngram_range=(1,1))\n",
        "vect.fit(df_train.texto)\n",
        "text_vect_train = vect.transform(df_train.texto)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "x_test = vect.transform(df_test.texto)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "t7Mx7bXKQjDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b17887e-432a-45a1-c02e-2ad866714db8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9805714285714285"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Contagem de termos simples com unigrama + brigrama e sem stop-words.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stops = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# vetorização\n",
        "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops)\n",
        "vect.fit(df_train.texto)\n",
        "text_vect_train = vect.transform(df_train.texto)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "x_test = vect.transform(df_test.texto)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "lQnAI7jfQjA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e64e291-662b-4d27-b65e-cf5b43c25985"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9851428571428571"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3 TF-IDF com unigrama e sem stop-words.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stops = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# vetorização\n",
        "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True, stop_words=stops)\n",
        "vect.fit(df_train.texto)\n",
        "text_vect_train = vect.transform(df_train.texto)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "x_test = vect.transform(df_test.texto)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "ZE-yFOCYQi-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0346442b-0876-4e22-8a36-07549702b98f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.984"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install spacy\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "r9ELr6DCSdrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.4 TF-IDF com unigrama e sem stop-words em textos lematizados.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "stops = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# função de lematização completa do documento\n",
        "def lemmatizer_text(text):\n",
        "  sent = []\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "      sent.append(word.lemma_)\n",
        "  return \" \".join(sent)\n",
        "\n",
        "# aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_train['text_lemma'] = df_train.texto.apply(lemmatizer_text)\n",
        "\n",
        "# vetorização\n",
        "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True, stop_words=stops)\n",
        "vect.fit(df_train.text_lemma)\n",
        "text_vect_train = vect.transform(df_train.text_lemma)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "# aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_test['text_lemma'] = df_test.texto.apply(lemmatizer_text)\n",
        "x_test = vect.transform(df_test.text_lemma)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "TDNE8CsQSdrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952b4e85-f117-4fa9-d6a9-14d08f52e01e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9828571428571429"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra:\n",
        "# 2.5 Contagem de termos simples (BoW) com unigrama, sem stop-words (combinando Spacy e NLTK) em textos com apenas verbos lematizados.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# stopwords do SpaCy e NLTK combinadas\n",
        "stops = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
        "\n",
        "# função de lematização para os verbos do documento\n",
        "def lemmatizer_verbs(text):\n",
        "  sent = []\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "      if word.pos_ == \"VERB\":\n",
        "          sent.append(word.lemma_)\n",
        "      else:\n",
        "          sent.append(word.text)\n",
        "  return \" \".join(sent)\n",
        "\n",
        "# aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_train['text_lemma_verbs'] = df_train.texto.apply(lemmatizer_verbs)\n",
        "\n",
        "# vetorização\n",
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
        "vect.fit(df_train.text_lemma_verbs)\n",
        "text_vect_train = vect.transform(df_train.text_lemma_verbs)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "# aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_test['text_lemma_verbs'] = df_test.texto.apply(lemmatizer_verbs)\n",
        "x_test = vect.transform(df_test.text_lemma_verbs)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "I27qsxndQi5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8e8618-03ac-4300-d8f3-cbc67938124c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9874285714285714"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}