{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 2 - Reinforcement Learning\n",
        "\n",
        "## Tutorial: Equa√ß√£o de Bellman\n",
        "\n",
        "### Prof. Dr. Ahirton Lopes"
      ],
      "metadata": {
        "id": "Rsz0bs3CtA3n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaGPgmzylA4r"
      },
      "source": [
        "# Equa√ß√£o de Bellman\n",
        "\n",
        "A Equa√ß√£o de Bellman √© um conceito fundamental no campo do Aprendizado por Refor√ßo, que √© uma abordagem de aprendizado de m√°quina em que um agente aprende a tomar decis√µes interativas para maximizar suas recompensas ao interagir com um ambiente. A equa√ß√£o leva o nome do matem√°tico Richard Bellman, que contribuiu significativamente para a teoria dos processos de decis√£o estoc√°sticos.\n",
        "\n",
        "A Equa√ß√£o de Bellman no contexto do Aprendizado por Refor√ßo √© uma f√≥rmula que expressa como o valor esperado de um estado est√° relacionado aos valores esperados dos estados futuros, levando em considera√ß√£o as recompensas imediatas e as poss√≠veis transi√ß√µes de estado. Ela descreve como um agente deve avaliar o valor de um estado com base nas recompensas futuras esperadas, levando em conta as a√ß√µes que o agente pode escolher.\n",
        "\n",
        "A formula√ß√£o b√°sica da Equa√ß√£o de Bellman √© a seguinte:\n",
        "\n",
        "`V(s) = max_a [ R(s, a) + Œ≥ * Œ£_s' [ P(s' | s, a) * V(s') ] ]`\n",
        "\n",
        "Onde:\n",
        "\n",
        "* V(s) √© o valor esperado do estado s.\n",
        "* max_a denota a maximiza√ß√£o sobre todas as a√ß√µes poss√≠veis a no estado s.\n",
        "* R(s, a) √© a recompensa imediata obtida ao executar a a√ß√£o a no estado s.\n",
        "* Œ≥ √© o fator de desconto que pondera as recompensas futuras em rela√ß√£o √†s recompensas imediatas.\n",
        "* P(s' | s, a) √© a probabilidade de transi√ß√£o para o estado s' a partir do estado s ao executar a a√ß√£o a.\n",
        "* V(s') √© o valor esperado do estado futuro s'.\n",
        "\n",
        "Em resumo, a Equa√ß√£o de Bellman √© uma ferramenta crucial para avaliar o valor de diferentes estados e guiar as decis√µes do agente para maximizar suas recompensas ao longo do tempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FLwYB6LlA4w"
      },
      "source": [
        "# Importar bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8Fc1C7u9lA4x"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0cEb_tFlA4y"
      },
      "source": [
        "# Definir Par√¢metros do Jogo\n",
        "\n",
        "Aqui, definimos os par√¢metros b√°sicos do jogo, incluindo o tamanho do grid, o estado inicial do agente, o estado do objetivo (moeda) e a taxa de desconto (gamma) usada na equa√ß√£o de Bellman."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hBgNm3f1lA4z"
      },
      "outputs": [],
      "source": [
        "rows = 4\n",
        "cols = 4\n",
        "start_state = (0, 0)\n",
        "goal_state = (3, 3)\n",
        "gamma = 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f95oP7clA4z"
      },
      "source": [
        "# Inicializar Valores de Estado (V)\n",
        "\n",
        "Inicializamos uma matriz V de tamanho (rows, cols) com zeros. Essa matriz representa os valores de estado do agente, ou seja, a estimativa de recompensa que o agente espera receber a partir de cada estado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gY7JU1uHlA4z"
      },
      "outputs": [],
      "source": [
        "V = np.zeros((rows, cols))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx34akI0lA4z"
      },
      "source": [
        "# Definir Fun√ß√£o de Recompensa\n",
        "\n",
        "Aqui, definimos uma fun√ß√£o de recompensa que retorna 10 se o estado for o estado do objetivo (moeda) e -1 caso contr√°rio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u5kUgSTxlA40"
      },
      "outputs": [],
      "source": [
        "def reward(state):\n",
        "    if state == goal_state:\n",
        "        return 10\n",
        "    else:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unjKchNslA40"
      },
      "source": [
        "# Definir Movimentos Poss√≠veis\n",
        "\n",
        "Criamos uma lista de a√ß√µes poss√≠veis que o agente pode tomar: mover-se para cima, para baixo, para a esquerda ou para a direita."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WQtpfgbnlA40"
      },
      "outputs": [],
      "source": [
        "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As altera√ß√µes s√£o sempre no formato (ùëëùë•,ùëëùë¶)(dx,dy), onde:ùëëùë• representa a mudan√ßa na coordenada x (linha). ùëëùë¶ representa a mudan√ßa na coordenada y (coluna). Portanto, a escolha dos valores √© baseada na necessidade de mover-se na grade em rela√ß√£o √† posi√ß√£o atual."
      ],
      "metadata": {
        "id": "ZnYvhuQ-6S8J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz6M3L-FlA41"
      },
      "source": [
        "# Algoritmo de Aprendizado por Refor√ßo\n",
        "\n",
        "Neste passo, implementamos o algoritmo de aprendizado por refor√ßo usando a equa√ß√£o de Bellman. Realizamos itera√ß√µes para atualizar os valores de estado (V) com base nas recompensas esperadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iU7f6lDHlA41"
      },
      "outputs": [],
      "source": [
        "num_iterations = 50000  # N√∫mero de itera√ß√µes do algoritmo de aprendizado por refor√ßo\n",
        "\n",
        "# Loop principal que executa o algoritmo de aprendizado por refor√ßo v√°rias vezes\n",
        "for _ in range(num_iterations):\n",
        "    new_V = np.copy(V)  # Cria uma c√≥pia dos valores de estado atuais para atualiza√ß√£o\n",
        "\n",
        "    # Loop para percorrer cada posi√ß√£o no grid\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            state = (i, j)  # Define o estado atual (posi√ß√£o atual no grid)\n",
        "\n",
        "            # Verifica se o estado atual √© o estado do objetivo\n",
        "            if state == goal_state:\n",
        "                continue  # Se for, pula para a pr√≥xima itera√ß√£o (n√£o faz nada neste estado)\n",
        "\n",
        "            max_value = float(\"-inf\")  # Inicializa o valor m√°ximo com um valor negativo infinito\n",
        "\n",
        "            # Loop para percorrer cada a√ß√£o poss√≠vel (movimento) no estado atual\n",
        "            for action in actions:\n",
        "                new_i = i + action[0]  # Calcula a nova linha ap√≥s a a√ß√£o\n",
        "                new_j = j + action[1]  # Calcula a nova coluna ap√≥s a a√ß√£o\n",
        "\n",
        "                # Verifica se a nova posi√ß√£o est√° dentro dos limites do grid\n",
        "                if 0 <= new_i < rows and 0 <= new_j < cols:\n",
        "                    new_state = (new_i, new_j)  # Define o novo estado ap√≥s a a√ß√£o\n",
        "                    value = reward(state) + gamma * V[new_state[0], new_state[1]]\n",
        "                    # Calcula o valor atualizado do estado usando a equa√ß√£o de Bellman\n",
        "                    # (recompensa imediata + fator de desconto * valor do novo estado)\n",
        "                    max_value = max(max_value, value)  # Mant√©m o valor m√°ximo encontrado\n",
        "\n",
        "            new_V[i, j] = max_value  # Atualiza o valor do estado atual na nova matriz de valores\n",
        "\n",
        "    V = new_V  # Atualiza a matriz de valores de estado com os novos valores calculados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuGqiSEXlA41"
      },
      "source": [
        "# Definir a Pol√≠tica do Agente\n",
        "\n",
        "Aqui, definimos a pol√≠tica do agente, ou seja, a a√ß√£o que o agente deve escolher em cada estado para maximizar sua recompensa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nnYA0TxslA41"
      },
      "outputs": [],
      "source": [
        "# Inicializa√ß√£o da matriz de pol√≠tica com valores vazios (\"\")\n",
        "policy = np.zeros((rows, cols), dtype=str)\n",
        "\n",
        "# Loop para iterar pelas linhas do grid\n",
        "for i in range(rows):\n",
        "    # Loop para iterar pelas colunas do grid\n",
        "    for j in range(cols):\n",
        "        # Verifica se a posi√ß√£o atual √© o estado objetivo\n",
        "        if (i, j) == goal_state:\n",
        "            # Se for o estado objetivo, atribui \"G\" √† pol√≠tica nessa posi√ß√£o\n",
        "            policy[i, j] = \"G\"\n",
        "        else:\n",
        "            # Inicializa o valor m√°ximo como negativo infinito\n",
        "            max_value = float(\"-inf\")\n",
        "            # Inicializa a a√ß√£o √≥tima como vazia (None)\n",
        "            best_action = None\n",
        "            # Loop para iterar pelas poss√≠veis a√ß√µes\n",
        "            for action in actions:\n",
        "                # Calcula a nova posi√ß√£o ap√≥s a a√ß√£o\n",
        "                new_i = i + action[0]\n",
        "                new_j = j + action[1]\n",
        "                # Verifica se a nova posi√ß√£o √© v√°lida dentro dos limites do grid\n",
        "                if 0 <= new_i < rows and 0 <= new_j < cols:\n",
        "                    # Calcula o novo estado ap√≥s a a√ß√£o\n",
        "                    new_state = (new_i, new_j)\n",
        "                    # Calcula o valor usando a Equa√ß√£o de Bellman para a nova posi√ß√£o\n",
        "                    value = reward((i, j)) + gamma * V[new_state[0], new_state[1]]\n",
        "                    # Atualiza o valor m√°ximo e a a√ß√£o √≥tima se o valor for maior\n",
        "                    if value > max_value:\n",
        "                        max_value = value\n",
        "                        best_action = action\n",
        "            # Atribui um s√≠mbolo √† pol√≠tica com base na a√ß√£o √≥tima encontrada\n",
        "            if best_action == (-1, 0):\n",
        "                policy[i, j] = \"‚Üë\"  # A√ß√£o de mover para cima\n",
        "            elif best_action == (1, 0):\n",
        "                policy[i, j] = \"‚Üì\"  # A√ß√£o de mover para baixo\n",
        "            elif best_action == (0, -1):\n",
        "                policy[i, j] = \"‚Üê\"  # A√ß√£o de mover para esquerda\n",
        "            elif best_action == (0, 1):\n",
        "                policy[i, j] = \"‚Üí\"  # A√ß√£o de mover para direita\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9FzSRiUlA42"
      },
      "source": [
        "#  Imprimir Valores Aprendidos para Cada Estado\n",
        "\n",
        "Este trecho de c√≥digo imprime os valores de estado aprendidos para cada posi√ß√£o no grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWbpLdOblA42",
        "outputId": "348190e9-71dd-4946-f5ae-168fe4d7afa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V(0,0): -4.69\tV(0,1): -4.10\tV(0,2): -3.44\tV(0,3): -2.71\t\n",
            "V(1,0): -4.10\tV(1,1): -3.44\tV(1,2): -2.71\tV(1,3): -1.90\t\n",
            "V(2,0): -3.44\tV(2,1): -2.71\tV(2,2): -1.90\tV(2,3): -1.00\t\n",
            "V(3,0): -2.71\tV(3,1): -1.90\tV(3,2): -1.00\tV(3,3): 0.00\t\n"
          ]
        }
      ],
      "source": [
        "for i in range(rows):\n",
        "    for j in range(cols):\n",
        "        print(f\"V({i},{j}): {V[i,j]:.2f}\", end=\"\\t\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWXkXH25lA42"
      },
      "source": [
        "# Imprimir a Pol√≠tica do Agente\n",
        "Este trecho de c√≥digo imprime a pol√≠tica final do agente, mostrando a a√ß√£o escolhida em cada posi√ß√£o do grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGpxb3X1lA42",
        "outputId": "a7d8ff47-9457-43d9-854c-1dd9b0b31b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚Üì\t‚Üì\t‚Üì\t‚Üì\t\n",
            "‚Üì\t‚Üì\t‚Üì\t‚Üì\t\n",
            "‚Üì\t‚Üì\t‚Üì\t‚Üì\t\n",
            "‚Üí\t‚Üí\t‚Üí\tG\t\n"
          ]
        }
      ],
      "source": [
        "for i in range(rows):\n",
        "    for j in range(cols):\n",
        "        print(policy[i, j], end=\"\\t\")\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}