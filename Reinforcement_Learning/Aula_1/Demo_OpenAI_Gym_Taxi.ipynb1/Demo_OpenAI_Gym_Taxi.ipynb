{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8lDQuQEnS8_"
      },
      "source": [
        "# Aula 1 - Reinforcement Learning\n",
        "\n",
        "## Tutorial: Uma introdução ao aprendizado por reforço usando o táxi do OpenAI Gym 🚕\n",
        "\n",
        "### Prof. Dr. Ahirton Lopes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziMM6qgmncE5"
      },
      "source": [
        "Neste tutorial introdutório, aplicaremos aprendizagem por reforço (RL) para treinar um agente para resolver o [ambiente 'Táxi' do OpenAI Gym](https://gymnasium.farama.org/environments/toy_text/taxi/).\n",
        "\n",
        "Abordaremos:\n",
        "\n",
        "- Uma introdução básica ao RL;\n",
        "- Configurando OpenAI Gym & Taxi;\n",
        "- Usando o algoritmo Q-learning para treinar nosso agente de táxi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfYSvJF6ocqk"
      },
      "source": [
        "# Antes de começarmos, o que é 'Taxi'?\n",
        "\n",
        "Táxi é um dos muitos ambientes disponíveis no OpenAI Gym. Esses ambientes são usados para desenvolver e avaliar algoritmos de aprendizagem por reforço.\n",
        "\n",
        "O objetivo do Táxi é pegar os passageiros e deixá-los no destino com o menor número de movimentos.\n",
        "\n",
        "Neste tutorial, vamos começar com um agente de táxi que executa ações aleatoriamente:\n",
        "\n",
        "![agente aleatório](https://drive.google.com/uc?id=1l0XizDh9eGP3gVNCjJHrC0M3DeCWI8Fj)\n",
        "\n",
        "…e aplicar com sucesso a aprendizagem por reforço para resolver o ambiente:\n",
        "\n",
        "![agente treinado](https://drive.google.com/uc?id=1a-OeLhXi3W-kvQuhGRyJ1dOSw4vrIBxr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZUF3oE-o889"
      },
      "source": [
        "# 💡 Uma introdução ao Aprendizado por Reforço\n",
        "\n",
        "Pense em como você pode ensinar um novo truque a um cachorro como, por exemplo,mandá-lo sentar:\n",
        "\n",
        "- Se ele executar o truque corretamente (sentar), você o recompensará com uma guloseima (feedback positivo) ✔️\n",
        "- Se não assentar corretamente, não recebe tratamento (feedback negativo) ❌\n",
        "\n",
        "Ao continuar a fazer coisas que levam a resultados positivos, o cão aprenderá a sentar-se ao ouvir o comando para receber a guloseima. O aprendizado por reforço é um subdomínio do aprendizado de máquina que envolve treinar um 'agente' (o cachorro) para aprender as sequências corretas de ações a serem executadas (sentado) em seu ambiente (em resposta ao comando 'sentar'), a fim de maximizar sua recompensa. (recebendo uma guloseima). Isso pode ser ilustrado mais formalmente como:\n",
        "\n",
        "![sutton barto rl](https://www.gocoder.one/static/RL-diagram-b3654cd3d5cc0e07a61a214977038f01.png \"Diagrama de aprendizado por reforço\")\n",
        "\n",
        "Fonte: [Sutton & Barto](http://incompleteideas.net/book/bookdraft2017nov5.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aWkaLM_o2pH"
      },
      "source": [
        "# 🏋️ Instalando OpenAI Gym e Taxi\n",
        "\n",
        "Usaremos o ambiente 'Taxi-v3' para este tutorial. Para instalar o gym (e numpy para depois), execute a célula abaixo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyjebRSHnK1F",
        "outputId": "010c3b4a-cb2b-46a1-e4eb-7b6081fd111a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in d:\\appdata\\anaconda3\\envs\\ai-env\\lib\\site-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in d:\\appdata\\anaconda3\\envs\\ai-env\\lib\\site-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\appdata\\anaconda3\\envs\\ai-env\\lib\\site-packages (from gym) (3.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in d:\\appdata\\anaconda3\\envs\\ai-env\\lib\\site-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy in d:\\appdata\\anaconda3\\envs\\ai-env\\lib\\site-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJK0BtorfEO"
      },
      "source": [
        "Em seguida, importe o gym (e bibliotecas adicionais que serão úteis posteriormente):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m7As7qh4navx"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# used to help with visualizing in Colab\n",
        "from IPython.display import display, clear_output\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyMhgh3RsLWk"
      },
      "source": [
        "Gym contém uma grande biblioteca de diferentes ambientes. Vamos criar o ambiente Taxi-v3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VpYHWA95y_QJ"
      },
      "outputs": [],
      "source": [
        "# create Taxi environment\n",
        "env = gym.make('Taxi-v3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWMjVABSsnWT"
      },
      "source": [
        "# 🎲 Agente aleatório\n",
        "\n",
        "Começaremos implementando um agente que não aprende nada. Em vez disso, selecionará ações aleatoriamente. Ele servirá como nosso *baseline*.\n",
        "\n",
        "O primeiro passo é dar ao nosso agente a observação inicial do estado. Um estado informa ao nosso agente como é o ambiente atual.\n",
        "\n",
        "No Táxi, um estado define as posições atuais do táxi, do passageiro e dos locais de embarque e desembarque. Abaixo estão exemplos de três estados diferentes para táxi:\n",
        "\n",
        "![estados de táxi](https://www.gocoder.one/static/taxi-states-0aad1b011cf3fe07b571712f2123335c.png \"Diferentes estados de táxi\")\n",
        "\n",
        "Nota: Amarelo = táxi, Letra azul = local de retirada, Letra roxa = destino de entrega\n",
        "\n",
        "Para obter o estado inicial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNlV-YvdnlOH",
        "outputId": "ddf5dfd7-bdb9-4a0b-9e32-227902dedc98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial state: (493, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n"
          ]
        }
      ],
      "source": [
        "# create a new instance of taxi, and get the initial state\n",
        "state = env.reset()\n",
        "\n",
        "print(f\"Initial state: {state}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM7GRNHnvRaH"
      },
      "source": [
        "A seguir, executaremos um loop for para percorrer o jogo. Em cada iteração, nosso agente irá:\n",
        "\n",
        "1. Fazer uma ação aleatória a partir do espaço de ação (0 - sul, 1 - norte, 2 - leste, 3 - oeste, 4 - recolha, 5 - desembarque)\n",
        "2. Receber o novo estado\n",
        "\n",
        "Aqui está nosso agente aleatório:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aycxOXzQnoLU",
        "outputId": "e84c94a0-cee4-42eb-e3e0-06e5eae98877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 0 out of 99\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "d:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\envs\\toy_text\\taxi.py:282: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"Taxi-v3\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "ename": "DependencyNotInstalled",
          "evalue": "pygame is not installed, run `pip install gym[toy_text]`",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\envs\\toy_text\\taxi.py:294\u001b[0m, in \u001b[0;36mTaxiEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m  \u001b[38;5;66;03m# dependency to pygame only if rendering with human\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# print the new state\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     sleep(\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# end this instance of the taxi environment\u001b[39;00m\n",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    327\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[0;32m    328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m     )\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:53\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_render_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:316\u001b[0m, in \u001b[0;36menv_render_passive_checker\u001b[1;34m(env, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[0;32m    312\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m         )\n\u001b[1;32m--> 316\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# TODO: Check that the result is correct\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\envs\\toy_text\\taxi.py:290\u001b[0m, in \u001b[0;36mTaxiEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\envs\\toy_text\\taxi.py:296\u001b[0m, in \u001b[0;36mTaxiEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m  \u001b[38;5;66;03m# dependency to pygame only if rendering with human\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpygame is not installed, run `pip install gym[toy_text]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m     )\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     pygame\u001b[38;5;241m.\u001b[39minit()\n",
            "\u001b[1;31mDependencyNotInstalled\u001b[0m: pygame is not installed, run `pip install gym[toy_text]`"
          ]
        }
      ],
      "source": [
        "num_steps = 99\n",
        "for s in range(num_steps+1):\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    print(f\"step: {s} out of {num_steps}\")\n",
        "\n",
        "    # sample a random action from the list of available actions\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # perform this action on the environment\n",
        "    env.step(action)\n",
        "\n",
        "    # print the new state\n",
        "    env.render()\n",
        "\n",
        "    sleep(0.2)\n",
        "\n",
        "# end this instance of the taxi environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I-Whw1WxDra"
      },
      "source": [
        "Ao executar a célula acima, você verá seu agente fazendo movimentos aleatórios. Não é muito emocionante, mas espero que tenha ajudado você a se familiarizar com o kit de ferramentas OpenAI Gym.\n",
        "\n",
        "A seguir, implementaremos o algoritmo Q-learning que permitirá ao nosso agente aprender com as recompensas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcmS8OwLyDL5"
      },
      "source": [
        "# 📖 Agente Q-Learning\n",
        "\n",
        "Q-learning é um algoritmo de aprendizagem por reforço que busca encontrar a melhor próxima ação possível dado seu estado atual, a fim de maximizar a recompensa que recebe (o 'Q' em Q-learning significa qualidade - ou seja, quão valiosa é uma ação) .\n",
        "\n",
        "Vamos considerar o seguinte estado inicial:\n",
        "\n",
        "![estado do táxi](https://www.gocoder.one/static/start-state-6a115a72f07cea072c28503d3abf9819.png \"Um exemplo de estado do táxi\")\n",
        "\n",
        "Que ação (para cima, para baixo, para a esquerda, para a direita, para pegar ou largar) ele deve realizar para maximizar sua recompensa? (_Nota: azul = local de retirada e roxo = destino de entrega_)\n",
        "\n",
        "Primeiro, vamos dar uma olhada em como nosso agente é “recompensado” por suas ações. **Lembre-se de que, no aprendizado por reforço, queremos que nosso agente execute ações que maximizem as possíveis recompensas que ele recebe de seu ambiente.**\n",
        "\n",
        "## Sistema de recompensas \"Táxi\"\n",
        "\n",
        "De acordo com a [documentação do táxi](https://gymnasium.farama.org/environments/toy_text/taxi/):\n",
        "\n",
        "> _\"…você recebe +20 pontos por uma entrega bem-sucedida e perde 1 ponto para cada intervalo de tempo necessário. Há também uma penalidade de 10 pontos para ações ilegais de coleta e entrega.\"_\n",
        "\n",
        "Olhando para o nosso estado original, as ações possíveis que ele pode realizar e as recompensas correspondentes que receberá são mostradas abaixo:\n",
        "\n",
        "![recompensas de táxi](https://www.gocoder.one/static/state-rewards-62ab43a53e07062b531b3199a8bab5b3.png \"Recompensas de táxi\")\n",
        "\n",
        "Na imagem acima, o agente perde 1 ponto por timestep que realiza. Ele também perderá 10 pontos se usar a ação de retirada ou entrega aqui.\n",
        "\n",
        "Queremos que nosso agente vá para o norte em direção ao local de coleta indicado por um R azul- **mas como ele saberá qual ação tomar se todos forem igualmente punitivos?**\n",
        "\n",
        "## Exploração (Exploration)\n",
        "\n",
        "Atualmente, nosso agente não tem como saber qual ação o levará mais próximo do R azul. É aqui que entra a tentativa e erro - faremos nosso agente realizar ações aleatórias e observar quais recompensas ele recebe (ou seja, nosso agente irá **explorar**).\n",
        "\n",
        "Ao longo de muitas iterações, nosso agente terá observado que certas sequências de ações serão mais gratificantes que outras. Ao longo do caminho, nosso agente precisará acompanhar quais ações levaram a quais recompensas.\n",
        "\n",
        "## Apresentando… tabelas Q\n",
        "\n",
        "Uma tabela Q é simplesmente uma tabela de consulta que armazena valores que representam as recompensas futuras máximas esperadas que nosso agente pode esperar para uma determinada ação em um determinado estado (_conhecidos como valores Q_). Isso dirá ao nosso agente que, quando ele encontra um determinado estado, algumas ações têm maior probabilidade do que outras de levar a recompensas mais altas. Torna-se uma 'folha de dicas' informando ao nosso agente qual é a melhor ação a ser tomada.\n",
        "\n",
        "A imagem abaixo ilustra como será a nossa 'tabela Q':\n",
        "\n",
        "- Cada linha corresponde a um estado único no ambiente 'Táxi'\n",
        "- Cada coluna corresponde a uma ação que nosso agente pode realizar\n",
        "- Cada célula corresponde ao valor Q para esse par estado-ação - um valor Q mais alto significa uma recompensa máxima mais alta que nosso agente pode esperar obter se realizar essa ação naquele estado.\n",
        "\n",
        "![Tabela Q](https://www.gocoder.one/static/q-table-9461cc903f50b78d757ea30aeb3eb8bc.png \"Tabela Q\")\n",
        "\n",
        "Antes de começarmos a treinar nosso agente, precisaremos inicializar nossa tabela Q da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVvgYgquUKBg",
        "outputId": "2d527b7f-cb9e-40fb-b961-8a8791901915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q table: [[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "state_size = env.observation_space.n  # total number of states (S)\n",
        "action_size = env.action_space.n      # total number of actions (A)\n",
        "\n",
        "# initialize a qtable with 0's for all Q-values\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "print(f\"Q table: {qtable}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0uvTrFXmJk7"
      },
      "source": [
        "À medida que nosso agente explora, ele atualizará a tabela Q com os valores Q que encontrar. Para calcular nossos valores Q, apresentaremos o algoritmo Q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdpsBFdJm9ve"
      },
      "source": [
        "# Algoritmo Q-Learning\n",
        "\n",
        "O algoritmo Q-learning é fornecido abaixo. Não entraremos em detalhes, mas você pode ler mais sobre isso no [Capítulo 6 de Sutton & Barto (2018)](http://www.incompleteideas.net/book/RLbook2018trimmed.pdf).\n",
        "\n",
        "![Algoritmo de aprendizagem Q](https://www.gocoder.one/static/q-learning-algorithm-84b84bb5dc16ba8097e31aff7ea42748.png \"O algoritmo de aprendizagem Q\")\n",
        "\n",
        "O algoritmo Q-learning ajudará nosso agente a **atualizar o valor Q atual ($Q(S_t,A_t)$) com suas observações após realizar uma ação.** Ou seja, aumente Q se encontrar uma recompensa positiva ou diminua Q se encontrar uma recompensa negativa.\n",
        "\n",
        "Observe que no Táxi, nosso agente não recebe uma recompensa positiva até que deixe um passageiro com sucesso (_+20 pontos_). Portanto, mesmo que nosso agente esteja indo na direção correta, haverá um atraso na recompensa positiva que deveria receber. O seguinte termo na equação Q-learning aborda isso:\n",
        "\n",
        "![q máximo](https://www.gocoder.one/static/max-q-e593ddcec76cda87ed189c31d60837b6.png \"Valor máximo de Q\")\n",
        "\n",
        "Este termo ajusta nosso valor Q atual para incluir uma parte das recompensas que ele poderá receber em algum momento no futuro (St+1). O termo 'a' refere-se a todas as ações possíveis disponíveis para esse estado. A equação também contém dois hiperparâmetros que podemos especificar:\n",
        "\n",
        "1. Taxa de aprendizagem (α): quão facilmente o agente deve aceitar novas informações em vez de informações aprendidas anteriormente\n",
        "2. Fator de desconto (γ): quanto o agente deve levar em consideração as recompensas que poderá receber no futuro versus sua recompensa imediata\n",
        "\n",
        "Aqui está nossa implementação do algoritmo Q-learning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsOWYTX4VsDz",
        "outputId": "b16f4319-2f5d-4e44-85c0-63966a8afffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q-value for (state, action) pair (417, 2): 9.0\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters to tune\n",
        "learning_rate = 0.9\n",
        "discount_rate = 0.8\n",
        "\n",
        "# dummy variables\n",
        "reward = 10 # R_(t+1)\n",
        "state = env.observation_space.sample()      # S_t\n",
        "action = env.action_space.sample()          # A_t\n",
        "new_state = env.observation_space.sample()  # S_(t+1)\n",
        "\n",
        "# Qlearning algorithm: Q(s,a) := Q(s,a) + learning_rate * (reward + discount_rate * max Q(s',a') - Q(s,a))\n",
        "qtable[state, action] += learning_rate * (reward + discount_rate * np.max(qtable[new_state,:]) - qtable[state,action])\n",
        "\n",
        "print(f\"Q-value for (state, action) pair ({state}, {action}): {qtable[state,action]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx5IMA1idW9X"
      },
      "source": [
        "## Comparação entre Exploration e Exploitation (Trade Off)\n",
        "\n",
        "Podemos deixar nosso agente explorar para atualizar nossa tabela Q usando o algoritmo Q-learning. À medida que nosso agente aprende mais sobre o ambiente, podemos deixá-lo usar esse conhecimento para realizar ações mais otimizadas e convergir mais rapidamente - conhecido como **exploitation**.\n",
        "\n",
        "Durante o exploitation, nosso agente examinará sua tabela Q e selecionará a ação com o valor Q mais alto (em vez de uma ação aleatória). Com o tempo, nosso agente precisará explorar menos e, em vez disso, começar \"exploitar\" o que sabe.\n",
        "\n",
        "Aqui está nossa implementação de uma estratégia de exploration-exploitation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3aorBEvYdSLr"
      },
      "outputs": [],
      "source": [
        "# dummy variables\n",
        "episode = random.randint(0,500)\n",
        "qtable = np.random.randn(env.observation_space.sample(), env.action_space.sample())\n",
        "\n",
        "# hyperparameters\n",
        "epsilon = 1.0     # probability that our agent will explore\n",
        "decay_rate = 0.01 # of epsilon\n",
        "\n",
        "if random.uniform(0,1) < epsilon:\n",
        "    # explore\n",
        "    action = env.action_space.sample()\n",
        "else:\n",
        "    # exploit\n",
        "    action = np.argmax(qtable[state,:])\n",
        "\n",
        "# epsilon decreases exponentially --> our agent will explore less and less\n",
        "epsilon = np.exp(-decay_rate*episode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_zL2Vrd1yLv"
      },
      "source": [
        "No exemplo acima, definimos algum valor `épsilon` entre 0 e 1. Se `épsilon` for 0,7, há 70% de chance de que nesta etapa nosso agente explore em vez de exploit. `epsilon` decai exponencialmente a cada passo, de modo que nosso agente explora cada vez menos ao longo do tempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5mA3SflarKs"
      },
      "source": [
        "# Reunindo tudo\n",
        "\n",
        "Concluímos todos os blocos de construção necessários para nosso agente de aprendizagem por reforço. O processo de treinamento do nosso agente será semelhante a:\n",
        "\n",
        "1. Inicializando nossa tabela Q com 0 para todos os valores Q\n",
        "2. Deixe nosso agente jogar Taxi em um grande número de jogos\n",
        "3. Atualizar continuamente a tabela Q usando o algoritmo Q-learning e uma estratégia de exploration-exploitation\n",
        "\n",
        "Aqui está a implementação completa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiXAK2OdpteR",
        "outputId": "f02b1686-db69-40a7-ed9a-7da845d5c5ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINED AGENT\n",
            "+++++EPISODE 3+++++\n",
            "Step 15\n",
            "\n",
            "Score: \u001b[32m6\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "class bcolors:\n",
        "    RED= '\\u001b[31m'\n",
        "    GREEN= '\\u001b[32m'\n",
        "    RESET= '\\u001b[0m'\n",
        "\n",
        "# create Taxi environment\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "# initialize q-table\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.9\n",
        "discount_rate = 0.8\n",
        "epsilon = 1.0\n",
        "decay_rate= 0.005\n",
        "\n",
        "# training variables\n",
        "num_episodes = 2000\n",
        "max_steps = 99 # per episode\n",
        "\n",
        "print(\"AGENT IS TRAINING...\")\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "\n",
        "\t# Reset the environment\n",
        "\tstate = env.reset()\n",
        "\tstep = 0\n",
        "\tdone = False\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\n",
        "\t\t# Exploration-exploitation tradeoff\n",
        "\t\tif random.uniform(0,1) < epsilon:\n",
        "\t\t\t# Explore\n",
        "\t\t\taction = env.action_space.sample()\n",
        "\t\telse:\n",
        "\t\t\t# Exploit\n",
        "\t\t\taction = np.argmax(qtable[state,:])\n",
        "\n",
        "\t\t# Take an action and observe the reward\n",
        "\t\tnew_state, reward, done, info = env.step(action)\n",
        "\n",
        "\t\t# Q-learning algorithm\n",
        "\t\tqtable[state,action] = qtable[state,action] + learning_rate * (reward + discount_rate * np.max(qtable[new_state,:])-qtable[state,action])\n",
        "\n",
        "\t\t# Update to our new state\n",
        "\t\tstate = new_state\n",
        "\n",
        "\t\t# if done, finish episode\n",
        "\t\tif done == True:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t# Decrease epsilon\n",
        "\tepsilon = np.exp(-decay_rate*episode)\n",
        "\n",
        "# Get ready to watch our trained agent\n",
        "clear_output()\n",
        "print(f\"Our Q-table: {qtable}\")\n",
        "print(f\"Training completed over {num_episodes} episodes\")\n",
        "input(\"Press Enter to see our trained taxi agent\")\n",
        "sleep(1)\n",
        "clear_output()\n",
        "\n",
        "episodes_to_preview = 3\n",
        "for episode in range(episodes_to_preview):\n",
        "\n",
        "\t# Reset the environment\n",
        "\tstate = env.reset()\n",
        "\tstep = 0\n",
        "\tdone = False\n",
        "\tepisode_rewards = 0\n",
        "\n",
        "\tfor step in range(num_steps):\n",
        "\t\t# clear screen\n",
        "\t\tclear_output(wait=True)\n",
        "\n",
        "\t\tprint(f\"TRAINED AGENT\")\n",
        "\t\tprint(f\"+++++EPISODE {episode+1}+++++\")\n",
        "\t\tprint(f\"Step {step+1}\")\n",
        "\n",
        "\t\t# Exploit\n",
        "\t\taction = np.argmax(qtable[state,:])\n",
        "\n",
        "\t\t# Take an action and observe the reward\n",
        "\t\tnew_state, reward, done, info = env.step(action)\n",
        "\n",
        "\t\t# Accumulate our rewards\n",
        "\t\tepisode_rewards += reward\n",
        "\n",
        "\t\tenv.render()\n",
        "\t\tprint(\"\")\n",
        "\t\tif episode_rewards < 0:\n",
        "\t\t\tprint(f\"Score: {bcolors.RED}{episode_rewards}{bcolors.RESET}\")\n",
        "\t\telse:\n",
        "\t\t\tprint(f\"Score: {bcolors.GREEN}{episode_rewards}{bcolors.RESET}\")\n",
        "\t\tsleep(0.5)\n",
        "\n",
        "\t\t# Update to our new state\n",
        "\t\tstate = new_state\n",
        "\n",
        "\t\t# if done, finish episode\n",
        "\t\tif done == True:\n",
        "\t\t\tbreak\n",
        "\n",
        "# Close the Taxi environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_tmtN9koho8"
      },
      "source": [
        "# 👏 O que vem a seguir?\n",
        "\n",
        "Existem muitos outros ambientes disponíveis no OpenAI Gym para você experimentar (por exemplo, [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/)). Você também pode tentar otimizar a implementação acima para resolver o Táxi em menos passos.\n",
        "\n",
        "Alguns outros recursos úteis incluem:\n",
        "- [Série de palestras de aprendizagem por reforço DeepMind x UCL [2021]](https://deepmind.com/learning-resources/reinforcement-learning-series-2021)\n",
        "- [Uma (longa) espiada na aprendizagem por reforço](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html) por Lilian Weng\n",
        "- [Um bom artigo sobre RL e suas aplicações no mundo real](https://www.altexsoft.com/blog/datascience/reinforcement-learning-explained-overview-comparisons-and-applications-in-business/)\n",
        "- [Documentário completo do AlphaGo](https://www.youtube.com/watch?v=WXuK6gekU1Y) (no Youtube)\n",
        "- [Aprendizagem por Reforço](http://www.incompleteideas.net/book/RLbook2018trimmed.pdf) por Sutton e Barto\n",
        "- [Introdução prática ao aprendizado por reforço profundo](https://www.gocoder.one/blog/hands-on-introduction-to-deep-reinforcement-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCdT5x2wqjrF"
      },
      "source": [
        "# O que resolvemos via Reinforcement Learning?\n",
        "\n",
        "* Programação de elevador\n",
        "* Passeio de bicicleta\n",
        "* Direção de navio\n",
        "* Controle de biorreator\n",
        "* Controle de helicóptero de acrobacias\n",
        "* Programação de partidas de aeroporto\n",
        "* Regulamentação e preservação de ecossistemas\n",
        "* Futebol Robocup\n",
        "* Jogo de videogame (Atari, Starcraft...)\n",
        "* Jogo de Go"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
