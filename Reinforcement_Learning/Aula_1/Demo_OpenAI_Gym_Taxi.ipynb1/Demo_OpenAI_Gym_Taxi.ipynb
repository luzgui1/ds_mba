{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8lDQuQEnS8_"
      },
      "source": [
        "# Aula 1 - Reinforcement Learning\n",
        "\n",
        "## Tutorial: Uma introdu√ß√£o ao aprendizado por refor√ßo usando o t√°xi do OpenAI Gym üöï\n",
        "\n",
        "### Prof. Dr. Ahirton Lopes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziMM6qgmncE5"
      },
      "source": [
        "Neste tutorial introdut√≥rio, aplicaremos aprendizagem por refor√ßo (RL) para treinar um agente para resolver o [ambiente 'T√°xi' do OpenAI Gym](https://gymnasium.farama.org/environments/toy_text/taxi/).\n",
        "\n",
        "Abordaremos:\n",
        "\n",
        "- Uma introdu√ß√£o b√°sica ao RL;\n",
        "- Configurando OpenAI Gym & Taxi;\n",
        "- Usando o algoritmo Q-learning para treinar nosso agente de t√°xi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfYSvJF6ocqk"
      },
      "source": [
        "# Antes de come√ßarmos, o que √©¬†'Taxi'?\n",
        "\n",
        "T√°xi √© um dos muitos ambientes dispon√≠veis no OpenAI Gym. Esses ambientes s√£o usados para desenvolver e avaliar algoritmos de aprendizagem por refor√ßo.\n",
        "\n",
        "O objetivo do T√°xi √© pegar os passageiros e deix√°-los no destino com o menor n√∫mero de movimentos.\n",
        "\n",
        "Neste tutorial, vamos come√ßar com um agente de t√°xi que executa a√ß√µes aleatoriamente:\n",
        "\n",
        "![agente aleat√≥rio](https://drive.google.com/uc?id=1l0XizDh9eGP3gVNCjJHrC0M3DeCWI8Fj)\n",
        "\n",
        "‚Ä¶e aplicar com sucesso a aprendizagem por refor√ßo para resolver o ambiente:\n",
        "\n",
        "![agente treinado](https://drive.google.com/uc?id=1a-OeLhXi3W-kvQuhGRyJ1dOSw4vrIBxr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZUF3oE-o889"
      },
      "source": [
        "# üí° Uma introdu√ß√£o ao Aprendizado por Refor√ßo\n",
        "\n",
        "Pense em como voc√™ pode ensinar um novo truque a um cachorro como, por exemplo,mand√°-lo sentar:\n",
        "\n",
        "- Se ele executar o truque corretamente (sentar), voc√™ o recompensar√° com uma guloseima (feedback positivo) ‚úîÔ∏è\n",
        "- Se n√£o assentar corretamente, n√£o recebe tratamento (feedback negativo) ‚ùå\n",
        "\n",
        "Ao continuar a fazer coisas que levam a resultados positivos, o c√£o aprender√° a sentar-se ao ouvir o comando para receber a guloseima. O aprendizado por refor√ßo √© um subdom√≠nio do aprendizado de m√°quina que envolve treinar um 'agente' (o cachorro) para aprender as sequ√™ncias corretas de a√ß√µes a serem executadas (sentado) em seu ambiente (em resposta ao comando 'sentar'), a fim de maximizar sua recompensa. (recebendo uma guloseima). Isso pode ser ilustrado mais formalmente como:\n",
        "\n",
        "![sutton barto rl](https://www.gocoder.one/static/RL-diagram-b3654cd3d5cc0e07a61a214977038f01.png \"Diagrama de aprendizado por refor√ßo\")\n",
        "\n",
        "Fonte: [Sutton &¬†Barto](http://incompleteideas.net/book/bookdraft2017nov5.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aWkaLM_o2pH"
      },
      "source": [
        "# üèãÔ∏è Instalando OpenAI Gym e¬†Taxi\n",
        "\n",
        "Usaremos o ambiente 'Taxi-v3' para este tutorial. Para instalar o gym (e numpy para depois), execute a c√©lula abaixo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyjebRSHnK1F",
        "outputId": "010c3b4a-cb2b-46a1-e4eb-7b6081fd111a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in d:\\appdata\\anaconda3\\envs\\ai-env\\lib\\site-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in d:\\appdata\\anaconda3\\envs\\ai-env\\lib\\site-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\appdata\\anaconda3\\envs\\ai-env\\lib\\site-packages (from gym) (3.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in d:\\appdata\\anaconda3\\envs\\ai-env\\lib\\site-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy in d:\\appdata\\anaconda3\\envs\\ai-env\\lib\\site-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJK0BtorfEO"
      },
      "source": [
        "Em seguida, importe o gym (e bibliotecas adicionais que ser√£o √∫teis posteriormente):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m7As7qh4navx"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# used to help with visualizing in Colab\n",
        "from IPython.display import display, clear_output\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyMhgh3RsLWk"
      },
      "source": [
        "Gym cont√©m uma grande biblioteca de diferentes ambientes. Vamos criar o ambiente Taxi-v3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VpYHWA95y_QJ"
      },
      "outputs": [],
      "source": [
        "# create Taxi environment\n",
        "env = gym.make('Taxi-v3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWMjVABSsnWT"
      },
      "source": [
        "# üé≤ Agente¬†aleat√≥rio\n",
        "\n",
        "Come√ßaremos implementando um agente que n√£o aprende nada. Em vez disso, selecionar√° a√ß√µes aleatoriamente. Ele servir√° como nosso *baseline*.\n",
        "\n",
        "O primeiro passo √© dar ao nosso agente a observa√ß√£o inicial do estado. Um estado informa ao nosso agente como √© o ambiente atual.\n",
        "\n",
        "No T√°xi, um estado define as posi√ß√µes atuais do t√°xi, do passageiro e dos locais de embarque e desembarque. Abaixo est√£o exemplos de tr√™s estados diferentes para t√°xi:\n",
        "\n",
        "![estados de t√°xi](https://www.gocoder.one/static/taxi-states-0aad1b011cf3fe07b571712f2123335c.png \"Diferentes estados de t√°xi\")\n",
        "\n",
        "Nota: Amarelo = t√°xi, Letra azul = local de retirada, Letra roxa = destino de entrega\n",
        "\n",
        "Para obter o estado inicial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNlV-YvdnlOH",
        "outputId": "ddf5dfd7-bdb9-4a0b-9e32-227902dedc98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial state: (493, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n"
          ]
        }
      ],
      "source": [
        "# create a new instance of taxi, and get the initial state\n",
        "state = env.reset()\n",
        "\n",
        "print(f\"Initial state: {state}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM7GRNHnvRaH"
      },
      "source": [
        "A seguir, executaremos um loop for para percorrer o jogo. Em cada itera√ß√£o, nosso agente ir√°:\n",
        "\n",
        "1. Fazer uma a√ß√£o aleat√≥ria a partir do espa√ßo de a√ß√£o (0‚Ää-‚Ääsul, 1‚Ää-‚Äänorte, 2‚Ää-‚Ääleste, 3‚Ää-‚Ääoeste, 4‚Ää-‚Äärecolha, 5‚Ää-‚Äädesembarque)\n",
        "2. Receber o novo estado\n",
        "\n",
        "Aqui est√° nosso agente aleat√≥rio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aycxOXzQnoLU",
        "outputId": "e84c94a0-cee4-42eb-e3e0-06e5eae98877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 0 out of 99\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "d:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\envs\\toy_text\\taxi.py:282: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"Taxi-v3\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "ename": "DependencyNotInstalled",
          "evalue": "pygame is not installed, run `pip install gym[toy_text]`",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\envs\\toy_text\\taxi.py:294\u001b[0m, in \u001b[0;36mTaxiEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m  \u001b[38;5;66;03m# dependency to pygame only if rendering with human\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# print the new state\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     sleep(\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# end this instance of the taxi environment\u001b[39;00m\n",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    327\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[0;32m    328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m     )\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:53\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_render_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:316\u001b[0m, in \u001b[0;36menv_render_passive_checker\u001b[1;34m(env, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[0;32m    312\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m         )\n\u001b[1;32m--> 316\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# TODO: Check that the result is correct\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\envs\\toy_text\\taxi.py:290\u001b[0m, in \u001b[0;36mTaxiEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\AppData\\anaconda3\\envs\\ai-env\\Lib\\site-packages\\gym\\envs\\toy_text\\taxi.py:296\u001b[0m, in \u001b[0;36mTaxiEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m  \u001b[38;5;66;03m# dependency to pygame only if rendering with human\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpygame is not installed, run `pip install gym[toy_text]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m     )\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     pygame\u001b[38;5;241m.\u001b[39minit()\n",
            "\u001b[1;31mDependencyNotInstalled\u001b[0m: pygame is not installed, run `pip install gym[toy_text]`"
          ]
        }
      ],
      "source": [
        "num_steps = 99\n",
        "for s in range(num_steps+1):\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    print(f\"step: {s} out of {num_steps}\")\n",
        "\n",
        "    # sample a random action from the list of available actions\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # perform this action on the environment\n",
        "    env.step(action)\n",
        "\n",
        "    # print the new state\n",
        "    env.render()\n",
        "\n",
        "    sleep(0.2)\n",
        "\n",
        "# end this instance of the taxi environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I-Whw1WxDra"
      },
      "source": [
        "Ao executar a c√©lula acima, voc√™ ver√° seu agente fazendo movimentos aleat√≥rios. N√£o √© muito emocionante, mas espero que tenha ajudado voc√™ a se familiarizar com o kit de ferramentas OpenAI Gym.\n",
        "\n",
        "A seguir, implementaremos o algoritmo Q-learning que permitir√° ao nosso agente aprender com as recompensas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcmS8OwLyDL5"
      },
      "source": [
        "# üìñ Agente Q-Learning\n",
        "\n",
        "Q-learning √© um algoritmo de aprendizagem por refor√ßo que busca encontrar a melhor pr√≥xima a√ß√£o poss√≠vel dado seu estado atual, a fim de maximizar a recompensa que recebe (o 'Q' em Q-learning significa qualidade‚Ää-‚Ääou seja, qu√£o valiosa √© uma a√ß√£o) .\n",
        "\n",
        "Vamos considerar o seguinte estado inicial:\n",
        "\n",
        "![estado do t√°xi](https://www.gocoder.one/static/start-state-6a115a72f07cea072c28503d3abf9819.png \"Um exemplo de estado do t√°xi\")\n",
        "\n",
        "Que a√ß√£o (para cima, para baixo, para a esquerda, para a direita, para pegar ou largar) ele deve realizar para maximizar sua recompensa? (_Nota: azul = local de retirada e roxo = destino de entrega_)\n",
        "\n",
        "Primeiro, vamos dar uma olhada em como nosso agente √© ‚Äúrecompensado‚Äù por suas a√ß√µes. **Lembre-se de que, no aprendizado por refor√ßo, queremos que nosso agente execute a√ß√µes que maximizem as poss√≠veis recompensas que ele recebe de seu ambiente.**\n",
        "\n",
        "## Sistema de recompensas \"T√°xi\"\n",
        "\n",
        "De acordo com a [documenta√ß√£o do t√°xi](https://gymnasium.farama.org/environments/toy_text/taxi/):\n",
        "\n",
        "> _\"‚Ä¶voc√™ recebe +20 pontos por uma entrega bem-sucedida e perde 1 ponto para cada intervalo de tempo necess√°rio. H√° tamb√©m uma penalidade de 10 pontos para a√ß√µes ilegais de coleta e entrega.\"_\n",
        "\n",
        "Olhando para o nosso estado original, as a√ß√µes poss√≠veis que ele pode realizar e as recompensas correspondentes que receber√° s√£o mostradas abaixo:\n",
        "\n",
        "![recompensas de t√°xi](https://www.gocoder.one/static/state-rewards-62ab43a53e07062b531b3199a8bab5b3.png \"Recompensas de t√°xi\")\n",
        "\n",
        "Na imagem acima, o agente perde 1 ponto por timestep que realiza. Ele tamb√©m perder√° 10 pontos se usar a a√ß√£o de retirada ou entrega aqui.\n",
        "\n",
        "Queremos que nosso agente v√° para o norte em dire√ß√£o ao local de coleta indicado por um R azul- **mas como ele saber√° qual a√ß√£o tomar se todos forem igualmente punitivos?**\n",
        "\n",
        "## Explora√ß√£o (Exploration)\n",
        "\n",
        "Atualmente, nosso agente n√£o tem como saber qual a√ß√£o o levar√° mais pr√≥ximo do R azul. √â aqui que entra a tentativa e erro - faremos nosso agente realizar a√ß√µes aleat√≥rias e observar quais recompensas ele recebe (ou seja, nosso agente ir√° **explorar**).\n",
        "\n",
        "Ao longo de muitas itera√ß√µes, nosso agente ter√° observado que certas sequ√™ncias de a√ß√µes ser√£o mais gratificantes que outras. Ao longo do caminho, nosso agente precisar√° acompanhar quais a√ß√µes levaram a quais recompensas.\n",
        "\n",
        "## Apresentando‚Ä¶ tabelas Q\n",
        "\n",
        "Uma tabela Q √© simplesmente uma tabela de consulta que armazena valores que representam as recompensas futuras m√°ximas esperadas que nosso agente pode esperar para uma determinada a√ß√£o em um determinado estado (_conhecidos como valores Q_). Isso dir√° ao nosso agente que, quando ele encontra um determinado estado, algumas a√ß√µes t√™m maior probabilidade do que outras de levar a recompensas mais altas. Torna-se uma 'folha de dicas' informando ao nosso agente qual √© a melhor a√ß√£o a ser tomada.\n",
        "\n",
        "A imagem abaixo ilustra como ser√° a nossa 'tabela Q':\n",
        "\n",
        "- Cada linha corresponde a um estado √∫nico no ambiente 'T√°xi'\n",
        "- Cada coluna corresponde a uma a√ß√£o que nosso agente pode realizar\n",
        "- Cada c√©lula corresponde ao valor Q para esse par estado-a√ß√£o‚Ää-‚Ääum valor Q mais alto significa uma recompensa m√°xima mais alta que nosso agente pode esperar obter se realizar essa a√ß√£o naquele estado.\n",
        "\n",
        "![Tabela Q](https://www.gocoder.one/static/q-table-9461cc903f50b78d757ea30aeb3eb8bc.png \"Tabela Q\")\n",
        "\n",
        "Antes de come√ßarmos a treinar nosso agente, precisaremos inicializar nossa tabela Q da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVvgYgquUKBg",
        "outputId": "2d527b7f-cb9e-40fb-b961-8a8791901915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q table: [[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "state_size = env.observation_space.n  # total number of states (S)\n",
        "action_size = env.action_space.n      # total number of actions (A)\n",
        "\n",
        "# initialize a qtable with 0's for all Q-values\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "print(f\"Q table: {qtable}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0uvTrFXmJk7"
      },
      "source": [
        "√Ä medida que nosso agente explora, ele atualizar√° a tabela Q com os valores Q que encontrar. Para calcular nossos valores Q, apresentaremos o algoritmo Q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdpsBFdJm9ve"
      },
      "source": [
        "# Algoritmo Q-Learning\n",
        "\n",
        "O algoritmo Q-learning √© fornecido abaixo. N√£o entraremos em detalhes, mas voc√™ pode ler mais sobre isso no [Cap√≠tulo 6 de Sutton & Barto (2018)](http://www.incompleteideas.net/book/RLbook2018trimmed.pdf).\n",
        "\n",
        "![Algoritmo de aprendizagem Q](https://www.gocoder.one/static/q-learning-algorithm-84b84bb5dc16ba8097e31aff7ea42748.png \"O algoritmo de aprendizagem Q\")\n",
        "\n",
        "O algoritmo Q-learning ajudar√° nosso agente a **atualizar o valor Q atual ($Q(S_t,A_t)$) com suas observa√ß√µes ap√≥s realizar uma a√ß√£o.** Ou seja, aumente Q se encontrar uma recompensa positiva ou diminua Q se encontrar uma recompensa negativa.\n",
        "\n",
        "Observe que no T√°xi, nosso agente n√£o recebe uma recompensa positiva at√© que deixe um passageiro com sucesso (_+20 pontos_). Portanto, mesmo que nosso agente esteja indo na dire√ß√£o correta, haver√° um atraso na recompensa positiva que deveria receber. O seguinte termo na equa√ß√£o Q-learning aborda isso:\n",
        "\n",
        "![q m√°ximo](https://www.gocoder.one/static/max-q-e593ddcec76cda87ed189c31d60837b6.png \"Valor m√°ximo de Q\")\n",
        "\n",
        "Este termo ajusta nosso valor Q atual para incluir uma parte das recompensas que ele poder√° receber em algum momento no futuro (St+1). O termo 'a' refere-se a todas as a√ß√µes poss√≠veis dispon√≠veis para esse estado. A equa√ß√£o tamb√©m cont√©m dois hiperpar√¢metros que podemos especificar:\n",
        "\n",
        "1. Taxa de aprendizagem (Œ±): qu√£o facilmente o agente deve aceitar novas informa√ß√µes em vez de informa√ß√µes aprendidas anteriormente\n",
        "2. Fator de desconto (Œ≥): quanto o agente deve levar em considera√ß√£o as recompensas que poder√° receber no futuro versus sua recompensa imediata\n",
        "\n",
        "Aqui est√° nossa implementa√ß√£o do algoritmo Q-learning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsOWYTX4VsDz",
        "outputId": "b16f4319-2f5d-4e44-85c0-63966a8afffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q-value for (state, action) pair (417, 2): 9.0\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters to tune\n",
        "learning_rate = 0.9\n",
        "discount_rate = 0.8\n",
        "\n",
        "# dummy variables\n",
        "reward = 10 # R_(t+1)\n",
        "state = env.observation_space.sample()      # S_t\n",
        "action = env.action_space.sample()          # A_t\n",
        "new_state = env.observation_space.sample()  # S_(t+1)\n",
        "\n",
        "# Qlearning algorithm: Q(s,a) := Q(s,a) + learning_rate * (reward + discount_rate * max Q(s',a') - Q(s,a))\n",
        "qtable[state, action] += learning_rate * (reward + discount_rate * np.max(qtable[new_state,:]) - qtable[state,action])\n",
        "\n",
        "print(f\"Q-value for (state, action) pair ({state}, {action}): {qtable[state,action]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx5IMA1idW9X"
      },
      "source": [
        "## Compara√ß√£o entre Exploration e Exploitation (Trade Off)\n",
        "\n",
        "Podemos deixar nosso agente explorar para atualizar nossa tabela Q usando o algoritmo Q-learning. √Ä medida que nosso agente aprende mais sobre o ambiente, podemos deix√°-lo usar esse conhecimento para realizar a√ß√µes mais otimizadas e convergir mais rapidamente‚Ää-‚Ääconhecido como **exploitation**.\n",
        "\n",
        "Durante o exploitation, nosso agente examinar√° sua tabela Q e selecionar√° a a√ß√£o com o valor Q mais alto (em vez de uma a√ß√£o aleat√≥ria). Com o tempo, nosso agente precisar√° explorar menos e, em vez disso, come√ßar \"exploitar\" o que sabe.\n",
        "\n",
        "Aqui est√° nossa implementa√ß√£o de uma estrat√©gia de exploration-exploitation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3aorBEvYdSLr"
      },
      "outputs": [],
      "source": [
        "# dummy variables\n",
        "episode = random.randint(0,500)\n",
        "qtable = np.random.randn(env.observation_space.sample(), env.action_space.sample())\n",
        "\n",
        "# hyperparameters\n",
        "epsilon = 1.0     # probability that our agent will explore\n",
        "decay_rate = 0.01 # of epsilon\n",
        "\n",
        "if random.uniform(0,1) < epsilon:\n",
        "    # explore\n",
        "    action = env.action_space.sample()\n",
        "else:\n",
        "    # exploit\n",
        "    action = np.argmax(qtable[state,:])\n",
        "\n",
        "# epsilon decreases exponentially --> our agent will explore less and less\n",
        "epsilon = np.exp(-decay_rate*episode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_zL2Vrd1yLv"
      },
      "source": [
        "No exemplo acima, definimos algum valor `√©psilon` entre 0 e 1. Se `√©psilon` for 0,7, h√° 70% de chance de que nesta etapa nosso agente explore em vez de exploit. `epsilon` decai exponencialmente a cada passo, de modo que nosso agente explora cada vez menos ao longo do tempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5mA3SflarKs"
      },
      "source": [
        "# Reunindo tudo\n",
        "\n",
        "Conclu√≠mos todos os blocos de constru√ß√£o necess√°rios para nosso agente de aprendizagem por refor√ßo. O processo de treinamento do nosso agente ser√° semelhante a:\n",
        "\n",
        "1. Inicializando nossa tabela Q com 0 para todos os valores Q\n",
        "2. Deixe nosso agente jogar Taxi em um grande n√∫mero de jogos\n",
        "3. Atualizar continuamente a tabela Q usando o algoritmo Q-learning e uma estrat√©gia de exploration-exploitation\n",
        "\n",
        "Aqui est√° a implementa√ß√£o completa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiXAK2OdpteR",
        "outputId": "f02b1686-db69-40a7-ed9a-7da845d5c5ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINED AGENT\n",
            "+++++EPISODE 3+++++\n",
            "Step 15\n",
            "\n",
            "Score: \u001b[32m6\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "class bcolors:\n",
        "    RED= '\\u001b[31m'\n",
        "    GREEN= '\\u001b[32m'\n",
        "    RESET= '\\u001b[0m'\n",
        "\n",
        "# create Taxi environment\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "# initialize q-table\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.9\n",
        "discount_rate = 0.8\n",
        "epsilon = 1.0\n",
        "decay_rate= 0.005\n",
        "\n",
        "# training variables\n",
        "num_episodes = 2000\n",
        "max_steps = 99 # per episode\n",
        "\n",
        "print(\"AGENT IS TRAINING...\")\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "\n",
        "\t# Reset the environment\n",
        "\tstate = env.reset()\n",
        "\tstep = 0\n",
        "\tdone = False\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\n",
        "\t\t# Exploration-exploitation tradeoff\n",
        "\t\tif random.uniform(0,1) < epsilon:\n",
        "\t\t\t# Explore\n",
        "\t\t\taction = env.action_space.sample()\n",
        "\t\telse:\n",
        "\t\t\t# Exploit\n",
        "\t\t\taction = np.argmax(qtable[state,:])\n",
        "\n",
        "\t\t# Take an action and observe the reward\n",
        "\t\tnew_state, reward, done, info = env.step(action)\n",
        "\n",
        "\t\t# Q-learning algorithm\n",
        "\t\tqtable[state,action] = qtable[state,action] + learning_rate * (reward + discount_rate * np.max(qtable[new_state,:])-qtable[state,action])\n",
        "\n",
        "\t\t# Update to our new state\n",
        "\t\tstate = new_state\n",
        "\n",
        "\t\t# if done, finish episode\n",
        "\t\tif done == True:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t# Decrease epsilon\n",
        "\tepsilon = np.exp(-decay_rate*episode)\n",
        "\n",
        "# Get ready to watch our trained agent\n",
        "clear_output()\n",
        "print(f\"Our Q-table: {qtable}\")\n",
        "print(f\"Training completed over {num_episodes} episodes\")\n",
        "input(\"Press Enter to see our trained taxi agent\")\n",
        "sleep(1)\n",
        "clear_output()\n",
        "\n",
        "episodes_to_preview = 3\n",
        "for episode in range(episodes_to_preview):\n",
        "\n",
        "\t# Reset the environment\n",
        "\tstate = env.reset()\n",
        "\tstep = 0\n",
        "\tdone = False\n",
        "\tepisode_rewards = 0\n",
        "\n",
        "\tfor step in range(num_steps):\n",
        "\t\t# clear screen\n",
        "\t\tclear_output(wait=True)\n",
        "\n",
        "\t\tprint(f\"TRAINED AGENT\")\n",
        "\t\tprint(f\"+++++EPISODE {episode+1}+++++\")\n",
        "\t\tprint(f\"Step {step+1}\")\n",
        "\n",
        "\t\t# Exploit\n",
        "\t\taction = np.argmax(qtable[state,:])\n",
        "\n",
        "\t\t# Take an action and observe the reward\n",
        "\t\tnew_state, reward, done, info = env.step(action)\n",
        "\n",
        "\t\t# Accumulate our rewards\n",
        "\t\tepisode_rewards += reward\n",
        "\n",
        "\t\tenv.render()\n",
        "\t\tprint(\"\")\n",
        "\t\tif episode_rewards < 0:\n",
        "\t\t\tprint(f\"Score: {bcolors.RED}{episode_rewards}{bcolors.RESET}\")\n",
        "\t\telse:\n",
        "\t\t\tprint(f\"Score: {bcolors.GREEN}{episode_rewards}{bcolors.RESET}\")\n",
        "\t\tsleep(0.5)\n",
        "\n",
        "\t\t# Update to our new state\n",
        "\t\tstate = new_state\n",
        "\n",
        "\t\t# if done, finish episode\n",
        "\t\tif done == True:\n",
        "\t\t\tbreak\n",
        "\n",
        "# Close the Taxi environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_tmtN9koho8"
      },
      "source": [
        "# üëè O que vem a seguir?\n",
        "\n",
        "Existem muitos outros ambientes dispon√≠veis no OpenAI Gym para voc√™ experimentar (por exemplo, [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/)). Voc√™ tamb√©m pode tentar otimizar a implementa√ß√£o acima para resolver o T√°xi em menos passos.\n",
        "\n",
        "Alguns outros recursos √∫teis incluem:\n",
        "- [S√©rie de palestras de aprendizagem por refor√ßo DeepMind x UCL [2021]](https://deepmind.com/learning-resources/reinforcement-learning-series-2021)\n",
        "- [Uma (longa) espiada na aprendizagem por refor√ßo](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html) por Lilian Weng\n",
        "- [Um bom artigo sobre RL e suas aplica√ß√µes no mundo real](https://www.altexsoft.com/blog/datascience/reinforcement-learning-explained-overview-comparisons-and-applications-in-business/)\n",
        "- [Document√°rio completo do AlphaGo](https://www.youtube.com/watch?v=WXuK6gekU1Y) (no Youtube)\n",
        "- [Aprendizagem por Refor√ßo](http://www.incompleteideas.net/book/RLbook2018trimmed.pdf) por Sutton e Barto\n",
        "- [Introdu√ß√£o pr√°tica ao aprendizado por refor√ßo profundo](https://www.gocoder.one/blog/hands-on-introduction-to-deep-reinforcement-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCdT5x2wqjrF"
      },
      "source": [
        "# O que resolvemos via Reinforcement Learning?\n",
        "\n",
        "* Programa√ß√£o de elevador\n",
        "* Passeio de bicicleta\n",
        "* Dire√ß√£o de navio\n",
        "* Controle de biorreator\n",
        "* Controle de helic√≥ptero de acrobacias\n",
        "* Programa√ß√£o de partidas de aeroporto\n",
        "* Regulamenta√ß√£o e preserva√ß√£o de ecossistemas\n",
        "* Futebol Robocup\n",
        "* Jogo de videogame (Atari, Starcraft...)\n",
        "* Jogo de Go"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
